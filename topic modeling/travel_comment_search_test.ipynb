{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "eb1f4014",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns in the dataset:\n",
      " - comment_id\n",
      " - original_comment\n",
      " - comment_no_emojis\n",
      " - emojis_in_comment\n",
      " - emoji_text_representation\n",
      " - detected_language\n",
      " - author_name\n",
      " - published_at\n",
      " - published_at_unix\n",
      " - like_count\n",
      "\n",
      "Preview of first 5 rows:\n",
      "                   comment_id  \\\n",
      "0  Ugy0Kpypj3U4drELURJ4AaABAg   \n",
      "1  UgyoE3fdtw3PKUElVhZ4AaABAg   \n",
      "2  UgzQ7ZTX6Z7qVSD9C-J4AaABAg   \n",
      "3  Ugz8x2qV2P8CqBoSzEJ4AaABAg   \n",
      "4  UgzFbBsDyBIV6rYe0AJ4AaABAg   \n",
      "\n",
      "                                    original_comment  \\\n",
      "0                      Anyone here 2025 may Thursday   \n",
      "1  <a href=\"https://www.youtube.com/watch?v=tOHIT...   \n",
      "2                   5-may-2025 whoâ€™s still listening   \n",
      "3                                   2025 now enjoy ðŸ¤   \n",
      "4                             Cristiano Ronaldo 2017   \n",
      "\n",
      "                                   comment_no_emojis emojis_in_comment  \\\n",
      "0                      Anyone here 2025 may Thursday               NaN   \n",
      "1  <a href=\"https://www.youtube.com/watch?v=tOHIT...               NaN   \n",
      "2                   5-may-2025 whoâ€™s still listening               NaN   \n",
      "3                                    2025 now enjoy                  ðŸ¤   \n",
      "4                             Cristiano Ronaldo 2017               NaN   \n",
      "\n",
      "  emoji_text_representation detected_language            author_name  \\\n",
      "0                       NaN                tl            @Embrace132   \n",
      "1                       NaN               und  @MuhammedKhaderAlAtra   \n",
      "2                       NaN                en         @HeanyOfficail   \n",
      "3             :white_heart:                pl  @ChalanRomeshanperera   \n",
      "4                       NaN                pt               @Apexxzz   \n",
      "\n",
      "           published_at  published_at_unix  like_count  \n",
      "0  2025-05-05T15:26:37Z         1746447997           0  \n",
      "1  2025-05-05T15:20:53Z         1746447653           0  \n",
      "2  2025-05-05T15:11:12Z         1746447072           0  \n",
      "3  2025-05-05T15:02:12Z         1746446532           1  \n",
      "4  2025-05-05T15:00:02Z         1746446402           0  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def inspect_columns(csv_path):\n",
    "    \"\"\"\n",
    "    Load a CSV file and print out its column names and a preview of the first few rows.\n",
    "    \"\"\"\n",
    "    # Load the CSV\n",
    "    df = pd.read_csv(csv_path, encoding='utf-8', low_memory=False)\n",
    "    \n",
    "    # Print column names\n",
    "    print(\"Columns in the dataset:\")\n",
    "    for col in df.columns:\n",
    "        print(f\" - {col}\")\n",
    "    \n",
    "    # Print a quick preview\n",
    "    print(\"\\nPreview of first 5 rows:\")\n",
    "    print(df.head())\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Update this path if needed\n",
    "    csv_path = r\"C:\\Users\\c-x-h\\OneDrive\\MasaÃ¼stÃ¼\\No_emoji_despa\\data\\despa_kJQP7kiw5Fk_comments\\despa_kJQP7kiw5Fk_comments.csv\"\n",
    "    inspect_columns(csv_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7b682eb2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting bs4\n",
      "  Downloading bs4-0.0.2-py2.py3-none-any.whl.metadata (411 bytes)\n",
      "Collecting beautifulsoup4 (from bs4)\n",
      "  Downloading beautifulsoup4-4.13.4-py3-none-any.whl.metadata (3.8 kB)\n",
      "Collecting soupsieve>1.2 (from beautifulsoup4->bs4)\n",
      "  Downloading soupsieve-2.7-py3-none-any.whl.metadata (4.6 kB)\n",
      "Requirement already satisfied: typing-extensions>=4.0.0 in d:\\a\\envs\\bertopic_env\\lib\\site-packages (from beautifulsoup4->bs4) (4.13.2)\n",
      "Downloading bs4-0.0.2-py2.py3-none-any.whl (1.2 kB)\n",
      "Downloading beautifulsoup4-4.13.4-py3-none-any.whl (187 kB)\n",
      "Downloading soupsieve-2.7-py3-none-any.whl (36 kB)\n",
      "Installing collected packages: soupsieve, beautifulsoup4, bs4\n",
      "Successfully installed beautifulsoup4-4.13.4 bs4-0.0.2 soupsieve-2.7\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install bs4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "06a91a36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting kneed\n",
      "  Downloading kneed-0.8.5-py3-none-any.whl.metadata (5.5 kB)\n",
      "Requirement already satisfied: numpy>=1.14.2 in d:\\a\\envs\\bertopic_env\\lib\\site-packages (from kneed) (2.2.5)\n",
      "Requirement already satisfied: scipy>=1.0.0 in d:\\a\\envs\\bertopic_env\\lib\\site-packages (from kneed) (1.15.2)\n",
      "Downloading kneed-0.8.5-py3-none-any.whl (10 kB)\n",
      "Installing collected packages: kneed\n",
      "Successfully installed kneed-0.8.5\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install kneed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ba1d2abe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns in dataset: ['comment_id', 'original_comment', 'comment_no_emojis', 'emojis_in_comment', 'emoji_text_representation', 'detected_language', 'author_name', 'published_at', 'published_at_unix', 'like_count']\n",
      "Rows after language filter: 1094704\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\c-x-h\\AppData\\Local\\Temp\\ipykernel_8468\\2383492124.py:57: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n",
      "  df_filtered = df[df['clean_text'].str.contains(pattern, na=False)].copy()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rows after keyword filter: 16136\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\a\\envs\\bertopic_env\\lib\\site-packages\\huggingface_hub\\file_download.py:144: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\c-x-h\\.cache\\huggingface\\hub\\models--sentence-transformers--paraphrase-multilingual-MiniLM-L12-v2. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
      "Batches: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 505/505 [04:18<00:00,  1.96it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detected similarity threshold: 0.703\n",
      "Similarity distribution saved to: C:\\Users\\c-x-h\\OneDrive\\MasaÃ¼stÃ¼\\similarity_distribution.png\n",
      "Number of comments selected: 22\n",
      "Filtered comments saved to: C:\\Users\\c-x-h\\OneDrive\\MasaÃ¼stÃ¼\\filtered_visit_comments.csv\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "\"\"\"\n",
    "Full pipeline for filtering and selecting â€œvisitâ€-related comments in English,\n",
    "using an existing 'comment_no_emojis' column (no emoji stripping needed).\n",
    "\n",
    "Requires:\n",
    "    pip install pandas beautifulsoup4 numpy scikit-learn matplotlib kneed sentence-transformers\n",
    "\"\"\"\n",
    "\n",
    "import pandas as pd\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import matplotlib.pyplot as plt\n",
    "from kneed import KneeLocator\n",
    "\n",
    "def preprocess_text(text: str) -> str:\n",
    "    \"\"\"\n",
    "    Remove URLs, HTML tags, and extra whitespace from text.\n",
    "    \"\"\"\n",
    "    # remove URLs\n",
    "    text = re.sub(r'http\\S+|www\\.\\S+', '', text)\n",
    "    # remove HTML tags using built-in parser\n",
    "    text = BeautifulSoup(text or \"\", \"html.parser\").get_text()\n",
    "    # collapse whitespace\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    return text\n",
    "\n",
    "def main():\n",
    "    # Paths\n",
    "    input_path  = r\"C:\\Users\\c-x-h\\OneDrive\\MasaÃ¼stÃ¼\\No_emoji_despa\\data\\despa_kJQP7kiw5Fk_comments\\despa_kJQP7kiw5Fk_comments.csv\"\n",
    "    output_path = r\"C:\\Users\\c-x-h\\OneDrive\\MasaÃ¼stÃ¼\\filtered_visit_comments.csv\"\n",
    "    plot_path   = r\"C:\\Users\\c-x-h\\OneDrive\\MasaÃ¼stÃ¼\\similarity_distribution.png\"\n",
    "\n",
    "    # 1. Load data\n",
    "    df = pd.read_csv(input_path, encoding='utf-8', low_memory=False)\n",
    "    print(\"Columns in dataset:\", df.columns.tolist())\n",
    "\n",
    "    # 2. Filter by language\n",
    "    if 'detected_language' not in df.columns:\n",
    "        raise KeyError(\"Column 'detected_language' not found.\")\n",
    "    df = df[df['detected_language'] == 'en'].copy()\n",
    "    print(f\"Rows after language filter: {len(df)}\")\n",
    "\n",
    "    # 3. Preprocess text using comment_no_emojis\n",
    "    if 'comment_no_emojis' not in df.columns:\n",
    "        raise KeyError(\"Column 'comment_no_emojis' not found.\")\n",
    "    df['clean_text'] = df['comment_no_emojis'].astype(str).apply(preprocess_text)\n",
    "\n",
    "    # 4. Regex-based keyword filtering\n",
    "    pattern = re.compile(\n",
    "        r'\\b(visit|travel|trip|go|place|wish I could|want to visit|Puerto Rico|Porto Rico|Porto Riko)\\b',\n",
    "        flags=re.IGNORECASE\n",
    "    )\n",
    "    df_filtered = df[df['clean_text'].str.contains(pattern, na=False)].copy()\n",
    "    print(f\"Rows after keyword filter: {len(df_filtered)}\")\n",
    "\n",
    "    # 5. Compute embeddings\n",
    "    model = SentenceTransformer('paraphrase-multilingual-MiniLM-L12-v2')\n",
    "    comments = df_filtered['clean_text'].tolist()\n",
    "    embeddings = model.encode(comments, show_progress_bar=True)\n",
    "\n",
    "    # 6. Seed sentence set & centroid\n",
    "    seed_sentences = [\n",
    "        \"I want to visit this place\",\n",
    "        \"I'd love to visit here\",\n",
    "        \"This looks like a place I should go\"\n",
    "    ]\n",
    "    seed_embeddings = model.encode(seed_sentences)\n",
    "    centroid = np.mean(seed_embeddings, axis=0, keepdims=True)\n",
    "\n",
    "    # 7. Cosine similarity\n",
    "    sims = cosine_similarity(embeddings, centroid).flatten()\n",
    "\n",
    "    # 8. Dynamic threshold via knee detection\n",
    "    sims_sorted = np.sort(sims)[::-1]\n",
    "    knee_locator = KneeLocator(\n",
    "        range(len(sims_sorted)), sims_sorted,\n",
    "        curve='convex', direction='decreasing'\n",
    "    )\n",
    "    knee = knee_locator.knee\n",
    "    threshold = sims_sorted[knee] if knee is not None else 0.7\n",
    "    print(f\"Detected similarity threshold: {threshold:.3f}\")\n",
    "\n",
    "    # 9. Save similarity distribution plot\n",
    "    plt.figure(figsize=(8, 4))\n",
    "    plt.plot(sims_sorted, label='Sorted Similarities')\n",
    "    if knee is not None:\n",
    "        plt.axvline(knee, color='red', linestyle='--', label=f'Knee at idx={knee}')\n",
    "    plt.title(\"Cosine Similarity Distribution\")\n",
    "    plt.xlabel(\"Ranked Comments\")\n",
    "    plt.ylabel(\"Similarity\")\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(plot_path)\n",
    "    plt.close()\n",
    "    print(f\"Similarity distribution saved to: {plot_path}\")\n",
    "\n",
    "    # 10. Select high-scoring comments and save\n",
    "    selected_idx = np.where(sims >= threshold)[0]\n",
    "    df_selected = df_filtered.iloc[selected_idx]\n",
    "    print(f\"Number of comments selected: {len(df_selected)}\")\n",
    "    df_selected.to_csv(output_path, index=False, encoding='utf-8')\n",
    "    print(f\"Filtered comments saved to: {output_path}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "471c971a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: Could not find a version that satisfies the requirement faiss (from versions: none)\n",
      "ERROR: No matching distribution found for faiss\n"
     ]
    }
   ],
   "source": [
    "pip install faiss "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "40f3d8bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns loaded: ['comment_id', 'original_comment', 'comment_no_emojis', 'emojis_in_comment', 'emoji_text_representation', 'detected_language', 'author_name', 'published_at', 'published_at_unix', 'like_count']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\c-x-h\\AppData\\Local\\Temp\\ipykernel_15392\\166916041.py:54: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n",
      "  df_kw = df[df['clean_text'].str.contains(pattern, na=False)].copy()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After broad regex filter: 27893 comments\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 872/872 [07:23<00:00,  1.96it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Threshold chosen: 0.600 (dynamic: 0.637, manual: 0.6)\n",
      "Similarity distribution plot saved to: C:\\Users\\c-x-h\\OneDrive\\MasaÃ¼stÃ¼\\similarity_distribution_extended.png\n",
      "Candidates after threshold filter: 173\n",
      "Candidates from clusters: 5474\n",
      "Candidates from Top-N (5000): 5000\n",
      "Total unique candidates: 8585\n",
      "Extended filtered comments saved to: C:\\Users\\c-x-h\\OneDrive\\MasaÃ¼stÃ¼\\extended_filtered_comments.csv\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "\"\"\"\n",
    "Extended pipeline to increase recall of â€œvisitâ€-related comments without FAISS:\n",
    "1. Broad regex keyword filtering\n",
    "2. Expanded seed set with per-seed max similarity\n",
    "3. Dynamic + manual threshold\n",
    "4. Clustering to capture thematic groups\n",
    "5. Top-N similarity retrieval instead of FAISS\n",
    "\n",
    "Requirements:\n",
    "    pip install pandas beautifulsoup4 numpy scikit-learn matplotlib sentence-transformers hdbscan kneed\n",
    "\"\"\"\n",
    "\n",
    "import pandas as pd\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.cluster import KMeans\n",
    "from kneed import KneeLocator\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def preprocess_text(text: str) -> str:\n",
    "    \"\"\"\n",
    "    Remove URLs, HTML tags, and extra whitespace.\n",
    "    \"\"\"\n",
    "    text = re.sub(r'http\\S+|www\\.\\S+', '', text)\n",
    "    text = BeautifulSoup(text or \"\", \"html.parser\").get_text()\n",
    "    return re.sub(r'\\s+', ' ', text).strip()\n",
    "\n",
    "def main():\n",
    "    # Paths\n",
    "    input_csv   = r\"C:\\Users\\c-x-h\\OneDrive\\MasaÃ¼stÃ¼\\No_emoji_despa\\data\\despa_kJQP7kiw5Fk_comments\\despa_kJQP7kiw5Fk_comments.csv\"\n",
    "    output_csv  = r\"C:\\Users\\c-x-h\\OneDrive\\MasaÃ¼stÃ¼\\extended_filtered_comments.csv\"\n",
    "    plot_path   = r\"C:\\Users\\c-x-h\\OneDrive\\MasaÃ¼stÃ¼\\similarity_distribution_extended.png\"\n",
    "\n",
    "    # 1. Load data\n",
    "    df = pd.read_csv(input_csv, encoding='utf-8', low_memory=False)\n",
    "    print(\"Columns loaded:\", df.columns.tolist())\n",
    "\n",
    "    # 2. Preprocess text\n",
    "    df['clean_text'] = df['comment_no_emojis'].astype(str).apply(preprocess_text)\n",
    "\n",
    "    # 3. Broad regex filtering\n",
    "    keywords = [\n",
    "        r'visit', r'travel', r'trip', r'go', r'place',\n",
    "        r'wish I could', r'want to visit', r'Puerto Rico',\n",
    "        r'Porto Rico', r'Porto Riko', r'vacation',\n",
    "        r'destination', r'sightseeing', r'getaway',\n",
    "        r'explore', r'backpack', r'itinerary'\n",
    "    ]\n",
    "    pattern = re.compile(r'\\b(' + '|'.join(keywords) + r')\\b', flags=re.IGNORECASE)\n",
    "    df_kw = df[df['clean_text'].str.contains(pattern, na=False)].copy()\n",
    "    print(f\"After broad regex filter: {len(df_kw)} comments\")\n",
    "\n",
    "    # 4. Compute embeddings\n",
    "    model = SentenceTransformer('paraphrase-multilingual-MiniLM-L12-v2')\n",
    "    texts = df_kw['clean_text'].tolist()\n",
    "    embeddings = model.encode(texts, show_progress_bar=True)\n",
    "    embeddings = np.array(embeddings, dtype=np.float32)\n",
    "\n",
    "    # 5. Seed sentences and embeddings\n",
    "    seed_sentences = [\n",
    "        \"I want to visit this place\",\n",
    "        \"I'd love to visit here\",\n",
    "        \"This looks like a place I should go\",\n",
    "        \"I've always wanted to go here\",\n",
    "        \"This destination is on my bucket list\",\n",
    "        \"Put this place on my must-visit list\",\n",
    "        \"Vacationing in this area\"\n",
    "    ]\n",
    "    seed_emb = model.encode(seed_sentences)\n",
    "    seed_emb = np.array(seed_emb, dtype=np.float32)\n",
    "\n",
    "    # 6. Per-seed similarity and maximum per comment\n",
    "    sims_per_seed = cosine_similarity(embeddings, seed_emb)  # shape: (n_comments, n_seeds)\n",
    "    max_sims = sims_per_seed.max(axis=1)\n",
    "\n",
    "    # 7. Determine threshold: dynamic knee + manual lower bound\n",
    "    sims_sorted = np.sort(max_sims)[::-1]\n",
    "    knee_idx = KneeLocator(range(len(sims_sorted)), sims_sorted,\n",
    "                           curve='convex', direction='decreasing').knee\n",
    "    dynamic_thresh = sims_sorted[knee_idx] if knee_idx is not None else 0.6\n",
    "    manual_thresh = 0.6\n",
    "    threshold = min(dynamic_thresh, manual_thresh)\n",
    "    print(f\"Threshold chosen: {threshold:.3f} (dynamic: {dynamic_thresh:.3f}, manual: {manual_thresh})\")\n",
    "\n",
    "    # Plot similarity distribution\n",
    "    plt.figure(figsize=(8,4))\n",
    "    plt.plot(sims_sorted, label='Max per-seed similarities')\n",
    "    if knee_idx is not None:\n",
    "        plt.axvline(knee_idx, color='red', linestyle='--', label=f'Knee idx={knee_idx}')\n",
    "    plt.title(\"Extended Cosine Similarity Distribution\")\n",
    "    plt.xlabel(\"Ranked Comments\")\n",
    "    plt.ylabel(\"Similarity\")\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(plot_path)\n",
    "    plt.close()\n",
    "    print(f\"Similarity distribution plot saved to: {plot_path}\")\n",
    "\n",
    "    # 8. Filter by threshold\n",
    "    idx_thresh = set(np.where(max_sims >= threshold)[0])\n",
    "    print(f\"Candidates after threshold filter: {len(idx_thresh)}\")\n",
    "\n",
    "    # 9. Clustering with KMeans to pick semantically similar clusters\n",
    "    n_clusters = 10\n",
    "    kmeans = KMeans(n_clusters=n_clusters, random_state=42).fit(embeddings)\n",
    "    centers = kmeans.cluster_centers_\n",
    "    avg_seed = seed_emb.mean(axis=0, keepdims=True)\n",
    "    cluster_sims = cosine_similarity(centers, avg_seed).flatten()\n",
    "    good_clusters = {i for i, sim in enumerate(cluster_sims) if sim >= threshold}\n",
    "    idx_clusters = {i for i, lbl in enumerate(kmeans.labels_) if lbl in good_clusters}\n",
    "    print(f\"Candidates from clusters: {len(idx_clusters)}\")\n",
    "\n",
    "    # 10. Top-N similarity retrieval to boost recall\n",
    "    top_n = 5000\n",
    "    top_indices = np.argsort(max_sims)[::-1][:top_n]\n",
    "    idx_topn = set(top_indices)\n",
    "    print(f\"Candidates from Top-N ({top_n}): {len(idx_topn)}\")\n",
    "\n",
    "    # 11. Combine all candidate indices\n",
    "    all_idx = idx_thresh.union(idx_clusters).union(idx_topn)\n",
    "    df_selected = df_kw.iloc[list(all_idx)].reset_index(drop=True)\n",
    "    print(f\"Total unique candidates: {len(df_selected)}\")\n",
    "\n",
    "    # 12. Save results\n",
    "    df_selected.to_csv(output_csv, index=False, encoding='utf-8')\n",
    "    print(f\"Extended filtered comments saved to: {output_csv}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "11eb09f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['comment_id', 'original_comment', 'comment_no_emojis', 'emojis_in_comment', 'emoji_text_representation', 'detected_language', 'author_name', 'published_at', 'published_at_unix', 'like_count']\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Replace backslashes with raw-string or double backslashes\n",
    "file_path = r\"C:\\Users\\c-x-h\\OneDrive\\MasaÃ¼stÃ¼\\No_emoji_despa\\data\\despa_kJQP7kiw5Fk_comments\\despa_kJQP7kiw5Fk_comments.csv\"\n",
    "\n",
    "# Read only the header row to get columns (faster for large files)\n",
    "df = pd.read_csv(file_path, nrows=0)\n",
    "\n",
    "# Print the list of column names\n",
    "print(df.columns.tolist())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "82f15be9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 304697 rows to search_2017\\comments_2017.csv\n",
      "Saved 187660 rows to search_2018\\comments_2018.csv\n",
      "Saved 157922 rows to search_2019\\comments_2019.csv\n",
      "Saved 215489 rows to search_2020\\comments_2020.csv\n",
      "Saved 118214 rows to search_2021\\comments_2021.csv\n",
      "Saved 49052 rows to search_2022\\comments_2022.csv\n",
      "Saved 26821 rows to search_2023\\comments_2023.csv\n",
      "Saved 26282 rows to search_2024\\comments_2024.csv\n",
      "Saved 8567 rows to search_2025\\comments_2025.csv\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# 1. Load the full dataset\n",
    "file_path = r\"C:\\Users\\c-x-h\\OneDrive\\MasaÃ¼stÃ¼\\No_emoji_despa\\data\\despa_kJQP7kiw5Fk_comments\\despa_kJQP7kiw5Fk_comments.csv\"\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# 2. Filter to English-language comments\n",
    "df_en = df[df['detected_language'] == 'en'].copy()\n",
    "\n",
    "# 3. Parse 'published_at' and extract year\n",
    "df_en['published_at'] = pd.to_datetime(df_en['published_at'])\n",
    "df_en['year'] = df_en['published_at'].dt.year\n",
    "\n",
    "# 4. For each year, save the two columns into its own folder\n",
    "for year in sorted(df_en['year'].unique()):\n",
    "    out_dir = f\"search_{year}\"\n",
    "    os.makedirs(out_dir, exist_ok=True)\n",
    "    \n",
    "    # Select only the two columns\n",
    "    df_year = df_en.loc[df_en['year'] == year, ['comment_no_emojis', 'published_at']]\n",
    "    \n",
    "    # Write to CSV\n",
    "    out_path = os.path.join(out_dir, f\"comments_{year}.csv\")\n",
    "    df_year.to_csv(out_path, index=False)\n",
    "    \n",
    "    print(f\"Saved {len(df_year)} rows to {out_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ce9c8e39",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\a\\envs\\bertopic_env\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Batches: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 101/101 [01:17<00:00,  1.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Year 2017: 304697 total â†’ 6439 word-filtered â†’ 100 top-100 â†’ 10 â‰¥ 0.6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 35/35 [00:26<00:00,  1.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Year 2018: 187660 total â†’ 2178 word-filtered â†’ 100 top-100 â†’ 8 â‰¥ 0.6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 33/33 [00:23<00:00,  1.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Year 2019: 157922 total â†’ 2077 word-filtered â†’ 100 top-100 â†’ 1 â‰¥ 0.6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 51/51 [00:43<00:00,  1.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Year 2020: 215489 total â†’ 3222 word-filtered â†’ 100 top-100 â†’ 3 â‰¥ 0.6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 25/25 [00:23<00:00,  1.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Year 2021: 118214 total â†’ 1568 word-filtered â†’ 100 top-100 â†’ 3 â‰¥ 0.6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:10<00:00,  1.07s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Year 2022: 49052 total â†’ 632 word-filtered â†’ 100 top-100 â†’ 1 â‰¥ 0.6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [00:04<00:00,  1.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Year 2023: 26821 total â†’ 288 word-filtered â†’ 100 top-100 â†’ 0 â‰¥ 0.6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:04<00:00,  1.15s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Year 2024: 26282 total â†’ 245 word-filtered â†’ 100 top-100 â†’ 1 â‰¥ 0.6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:02<00:00,  1.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Year 2025: 8567 total â†’ 107 word-filtered â†’ 100 top-100 â†’ 0 â‰¥ 0.6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "\n",
    "# â”€â”€â”€ Configuration â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# Path to your original CSV\n",
    "FILE_PATH = r\"C:\\Users\\c-x-h\\OneDrive\\MasaÃ¼stÃ¼\\No_emoji_despa\\data\\despa_kJQP7kiw5Fk_comments\\despa_kJQP7kiw5Fk_comments.csv\"\n",
    "\n",
    "# N-gram filtering keywords\n",
    "KEYWORDS = [\n",
    "    \"visit\", \"travel\", \"trip\", \"go\", \"place\",\n",
    "    \"wish i could\", \"want to visit\",\n",
    "    \"puerto rico\", \"porto rico\", \"porto riko\"\n",
    "]\n",
    "\n",
    "# Sentence-Transformer model and seed sentence\n",
    "MODEL_NAME    = \"all-MiniLM-L6-v2\"\n",
    "SEED_SENTENCE = \"I want to visit this place.\"\n",
    "\n",
    "# Semantic similarity parameters\n",
    "TOP_K      = 100\n",
    "THRESHOLD  = 0.6\n",
    "\n",
    "# â”€â”€â”€ Load & Preprocess â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "df = pd.read_csv(FILE_PATH)\n",
    "\n",
    "# 1. Keep only English comments\n",
    "df = df[df[\"detected_language\"] == \"en\"].copy()\n",
    "\n",
    "# 2. Parse dates & extract year\n",
    "df[\"published_at\"] = pd.to_datetime(df[\"published_at\"])\n",
    "df[\"year\"] = df[\"published_at\"].dt.year\n",
    "\n",
    "# â”€â”€â”€ Set up N-gram Vectorizer â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "vectorizer = CountVectorizer(\n",
    "    ngram_range=(1, 3),\n",
    "    vocabulary=KEYWORDS,\n",
    "    lowercase=True,\n",
    "    token_pattern=r\"(?u)\\b\\w+\\b\"\n",
    ")\n",
    "\n",
    "# â”€â”€â”€ Set up Semantic Model â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "model   = SentenceTransformer(MODEL_NAME)\n",
    "seed_emb = model.encode(SEED_SENTENCE, convert_to_tensor=True)\n",
    "\n",
    "# â”€â”€â”€ Process Year by Year â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "for year, group in df.groupby(\"year\"):\n",
    "    out_dir = f\"search_{year}\"\n",
    "    os.makedirs(out_dir, exist_ok=True)\n",
    "\n",
    "    # Save original comments for reference\n",
    "    comments_path = os.path.join(out_dir, f\"comments_{year}.csv\")\n",
    "    group[[\"comment_no_emojis\", \"published_at\"]].to_csv(comments_path, index=False)\n",
    "\n",
    "    # 1) Word- and Phrase-Based Filtering\n",
    "    X      = vectorizer.transform(group[\"comment_no_emojis\"])\n",
    "    mask   = (X.sum(axis=1).A1 > 0)\n",
    "    word_df = group.loc[mask, [\"comment_no_emojis\", \"published_at\"]].copy()\n",
    "\n",
    "    word_path = os.path.join(out_dir, f\"word_filtered_{year}.csv\")\n",
    "    word_df.to_csv(word_path, index=False)\n",
    "\n",
    "    # 2) Semantic Similarity Scoring\n",
    "    #    Embed the filtered comments (in batches)\n",
    "    comments = word_df[\"comment_no_emojis\"].tolist()\n",
    "    embeddings = model.encode(\n",
    "        comments,\n",
    "        convert_to_tensor=True,\n",
    "        batch_size=64,\n",
    "        show_progress_bar=True\n",
    "    )\n",
    "\n",
    "    #    Compute cosine similarity against the seed\n",
    "    sims = util.cos_sim(embeddings, seed_emb).squeeze(1)\n",
    "    word_df[\"similarity_score\"] = sims.cpu().numpy()\n",
    "\n",
    "    #    Sort by descending similarity\n",
    "    sorted_df = word_df.sort_values(\"similarity_score\", ascending=False)\n",
    "\n",
    "    # 2a) Top-K selection\n",
    "    topk_df = sorted_df.head(TOP_K)\n",
    "    topk_path = os.path.join(out_dir, f\"semantic_topk_{year}.csv\")\n",
    "    topk_df.to_csv(topk_path, index=False)\n",
    "\n",
    "    # 2b) Threshold filtering\n",
    "    thresh_df = sorted_df[sorted_df[\"similarity_score\"] >= THRESHOLD]\n",
    "    thresh_path = os.path.join(out_dir, f\"semantic_threshold_{year}.csv\")\n",
    "    thresh_df.to_csv(thresh_path, index=False)\n",
    "\n",
    "    print(\n",
    "        f\"Year {year}: \"\n",
    "        f\"{len(group)} total â†’ \"\n",
    "        f\"{len(word_df)} word-filtered â†’ \"\n",
    "        f\"{len(topk_df)} top-{TOP_K} â†’ \"\n",
    "        f\"{len(thresh_df)} â‰¥ {THRESHOLD}\"\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "06c456bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote 900 rows into combined_semantic_topk.xlsx\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import glob\n",
    "import os\n",
    "\n",
    "# â”€â”€â”€ Adjust this if your folders are elsewhere â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "BASE_PATTERN = \"Travel_Search/search_*/semantic_topk_*.csv\"\n",
    "\n",
    "# â”€â”€â”€ Read & combine â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "all_dfs = []\n",
    "for path in glob.glob(BASE_PATTERN):\n",
    "    df = pd.read_csv(path)\n",
    "    # Convert published_at to just the year (int or str)\n",
    "    df[\"published_at\"] = pd.to_datetime(df[\"published_at\"]).dt.year\n",
    "    all_dfs.append(df)\n",
    "\n",
    "if not all_dfs:\n",
    "    raise FileNotFoundError(f\"No files matched pattern {BASE_PATTERN}\")\n",
    "\n",
    "combined = pd.concat(all_dfs, ignore_index=True)\n",
    "\n",
    "# â”€â”€â”€ Write to Excel â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "output_path = \"combined_semantic_topk.xlsx\"\n",
    "with pd.ExcelWriter(output_path, engine=\"openpyxl\") as writer:\n",
    "    combined.to_excel(writer, index=False, sheet_name=\"TopK\")\n",
    "\n",
    "print(f\"Wrote {len(combined)} rows into {output_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9f89cf52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 9 files:\n",
      "   Travel_Search\\search_2017\\semantic_topk_2017.csv\n",
      "   Travel_Search\\search_2018\\semantic_topk_2018.csv\n",
      "   Travel_Search\\search_2019\\semantic_topk_2019.csv\n",
      "   Travel_Search\\search_2020\\semantic_topk_2020.csv\n",
      "   Travel_Search\\search_2021\\semantic_topk_2021.csv\n",
      "   Travel_Search\\search_2022\\semantic_topk_2022.csv\n",
      "   Travel_Search\\search_2023\\semantic_topk_2023.csv\n",
      "   Travel_Search\\search_2024\\semantic_topk_2024.csv\n",
      "   Travel_Search\\search_2025\\semantic_topk_2025.csv\n",
      "\n",
      "Wrote Top 200 and Top 500 to 'topk_200_500.xlsx'\n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# â”€â”€â”€ Configuration â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# Pattern to find your per-year semantic_topk files\n",
    "PATTERN = os.path.join(\"Travel_Search\",\"search_*\", \"semantic_topk_*.csv\")\n",
    "OUTPUT_FILE = \"topk_200_500.xlsx\"\n",
    "\n",
    "# â”€â”€â”€ Find files â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "paths = glob.glob(PATTERN)\n",
    "if not paths:\n",
    "    raise FileNotFoundError(f\"No files matched pattern: {PATTERN}\\n\"\n",
    "                            \"Make sure you're running this script from the folder containing your search_<year> directories.\")\n",
    "\n",
    "print(f\"Found {len(paths)} files:\")\n",
    "for p in paths:\n",
    "    print(\"  \", p)\n",
    "\n",
    "# â”€â”€â”€ Read & normalize â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "dfs = []\n",
    "for p in paths:\n",
    "    df = pd.read_csv(p)\n",
    "    # reduce published_at to just the year\n",
    "    df[\"published_at\"] = pd.to_datetime(df[\"published_at\"]).dt.year\n",
    "    dfs.append(df[[\"comment_no_emojis\", \"published_at\", \"similarity_score\"]])\n",
    "\n",
    "# â”€â”€â”€ Combine â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "combined = pd.concat(dfs, ignore_index=True)\n",
    "combined = combined.sort_values(\"similarity_score\", ascending=False)\n",
    "\n",
    "# â”€â”€â”€ Write Top-200 & Top-500 â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "with pd.ExcelWriter(OUTPUT_FILE, engine=\"openpyxl\") as writer:\n",
    "    combined.head(200).to_excel(writer, sheet_name=\"Top_200\", index=False)\n",
    "    combined.head(500).to_excel(writer, sheet_name=\"Top_500\", index=False)\n",
    "\n",
    "print(f\"\\nWrote Top 200 and Top 500 to '{OUTPUT_FILE}'\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bertopic_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
